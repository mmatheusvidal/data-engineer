{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lambda3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J2PuM1aB1ftm"
      },
      "outputs": [],
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')\n",
        "\n",
        "# iniciar uma sessão local e importar dados do Airbnb\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "GwxDaDO-14ts"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entregavel 2\n",
        "\n",
        "A primeira avaliação feita foi para buscar dados faltantes no dataset.\n",
        "Neste caso temos um dado de desconto faltante na transação 4, constatado isso, tomei a decisão de usar a função fillna para substituir o valor por 0 pois não afetaria nosso cálculo posterior.\n",
        "\n",
        "Como não temos nenhuma linha com o dado de total_bruto faltante não foi necessário fazer um filtro para isso. Caso existisse minha indicação seria criar uma validação para esse dado e um alerta na pipeline para que o possível erro possa ser investigado."
      ],
      "metadata": {
        "id": "IMOAeJ6u2hLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transacoes = [\n",
        "    {'transacao_id':1, 'total_bruto':3000, 'percentual_desconto': 6.99},\n",
        "    {'transacao_id':2, 'total_bruto':57989, 'percentual_desconto': 1.45},\n",
        "    {'transacao_id':4, 'total_bruto':1, 'percentual_desconto': None},\n",
        "    {'transacao_id':5, 'total_bruto':34, 'percentual_desconto': 0.0}\n",
        "    ]\n",
        "\n",
        "df_transacoes = spark.createDataFrame(transacoes)\n",
        "\n",
        "df_transacoes = df_transacoes.fillna(0)\n",
        "df_transacoes = df_transacoes.withColumn('res', df_transacoes['total_bruto'] - (df_transacoes['total_bruto'] * (df_transacoes['percentual_desconto']/100)))\n",
        "\n",
        "lucro = round(df_transacoes.select(F.sum('res')).collect()[0][0],2)\n",
        "\n",
        "print(f\"Lucro: R$ {lucro}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T1p10RU2kH4",
        "outputId": "ffbd7b79-20b1-4419-8e49-6f046dd8a6e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lucro: R$ 59973.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entregavel 3.1"
      ],
      "metadata": {
        "id": "n7P0_fn03Dmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "   {\n",
        "      \"CreateDate\":\"2021-05-24T20:21:34.79\",\n",
        "      \"EmissionDate\":\"2021-05-24T00:00:00\",\n",
        "      \"Discount\":0.0,\n",
        "      \"NFeNumber\":501,\n",
        "      \"NFeID\":1,\n",
        "      \"ItemList\":[\n",
        "         {\n",
        "            \"ProductName\":\"Rice\",\n",
        "            \"Value\":35.55,\n",
        "            \"Quantity\":2\n",
        "         },\n",
        "         {\n",
        "            \"ProductName\":\"Flour\",\n",
        "            \"Value\":11.55,\n",
        "            \"Quantity\":5\n",
        "         },\n",
        "         {\n",
        "            \"ProductName\":\"Bean\",\n",
        "            \"Value\":27.15,\n",
        "            \"Quantity\":7\n",
        "         }\n",
        "      ]\n",
        "   },\n",
        "   {\n",
        "      \"CreateDate\":\"2021-05-24T20:21:34.79\",\n",
        "      \"EmissionDate\":\"2021-05-24T00:00:00\",\n",
        "      \"Discount\":0.0,\n",
        "      \"NFeNumber\":502,\n",
        "      \"NFeID\":2,\n",
        "      \"ItemList\":[\n",
        "         {\n",
        "            \"ProductName\":\"Tomate\",\n",
        "            \"Value\":12.25,\n",
        "            \"Quantity\":10\n",
        "         },\n",
        "         {\n",
        "            \"ProductName\":\"Pasta\",\n",
        "            \"Value\":7.55,\n",
        "            \"Quantity\":5\n",
        "         }\n",
        "      ]\n",
        "   },\n",
        "   {\n",
        "      \"CreateDate\":\"2021-05-24T20:21:34.79\",\n",
        "      \"EmissionDate\":\"2021-05-24T00:00:00\",\n",
        "      \"Discount\":0.0,\n",
        "      \"NFeNumber\":503,\n",
        "      \"NFeID\":3,\n",
        "      \"ItemList\":[\n",
        "         {\n",
        "            \"ProductName\":\"Beer\",\n",
        "            \"Value\":9.00,\n",
        "            \"Quantity\":6\n",
        "         },\n",
        "         {\n",
        "            \"ProductName\":\"French fries\",\n",
        "            \"Value\":10.99,\n",
        "            \"Quantity\":2\n",
        "         },\n",
        "         {\n",
        "            \"ProductName\":\"Ice cream\",\n",
        "            \"Value\":27.15,\n",
        "            \"Quantity\":1\n",
        "         }\n",
        "      ]\n",
        "   }\n",
        "]"
      ],
      "metadata": {
        "id": "-V2U7kuc5asc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No primeiro entregável do item 3 optei por usar um modelo parecido com o modelo de OBT usando a quantidade de itens na lista para construir o DataFrame.\n",
        "minha decisão foi essa por entender que apenas usar o explode na coluna ItemList eu teria um resultado muito similar ao segundo entregavel descrito abaixo."
      ],
      "metadata": {
        "id": "gC7VUhepihw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data)\n",
        "df_tmp = df.withColumn('tamanho', F.size(df.ItemList))\n",
        "colunas = df_tmp.select(F.max(df_tmp.tamanho)).take(1)[0][0]\n",
        "\n",
        "for i in range(colunas):\n",
        "  df = df.withColumn(f'ValorProduto{i}', df.ItemList[i]['Value'])\n",
        "  df = df.withColumn(f'NomeProduto{i}', df.ItemList[i]['ProductName'])\n",
        "  df = df.withColumn(f'QuantityProduto{i}', df.ItemList[i]['Quantity'])\n",
        "\n",
        "df = df.drop(df.ItemList)\n",
        "\n",
        "df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4zw66DC1qu9",
        "outputId": "15efa392-904d-4238-98a7-0e22e0a219ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+-------------------+-----+---------+-------------+------------+----------------+-------------+------------+----------------+-------------+------------+----------------+\n",
            "|          CreateDate|Discount|       EmissionDate|NFeID|NFeNumber|ValorProduto0|NomeProduto0|QuantityProduto0|ValorProduto1|NomeProduto1|QuantityProduto1|ValorProduto2|NomeProduto2|QuantityProduto2|\n",
            "+--------------------+--------+-------------------+-----+---------+-------------+------------+----------------+-------------+------------+----------------+-------------+------------+----------------+\n",
            "|2021-05-24T20:21:...|     0.0|2021-05-24T00:00:00|    1|      501|        35.55|        Rice|               2|        11.55|       Flour|               5|        27.15|        Bean|               7|\n",
            "|2021-05-24T20:21:...|     0.0|2021-05-24T00:00:00|    2|      502|        12.25|      Tomate|              10|         7.55|       Pasta|               5|         null|        null|            null|\n",
            "|2021-05-24T20:21:...|     0.0|2021-05-24T00:00:00|    3|      503|          9.0|        Beer|               6|        10.99|French fries|               2|        27.15|   Ice cream|               1|\n",
            "+--------------------+--------+-------------------+-----+---------+-------------+------------+----------------+-------------+------------+----------------+-------------+------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entregavel 3.2\n",
        "\n",
        "Para o segundo entregável do item 3 optei por fazer a operação de explode na coluna ItemList e usar a função getItem para acessar os dados dentro do dicionário resultante.\n",
        "\n",
        "Com isso temos 2 tabelas como resultado do entregável\n",
        "\n",
        "- df_itens:\n",
        "  - CreateDate\n",
        "  - NFeNumber\n",
        "  - Value\n",
        "  - Quantity\n",
        "  - ProductName\n",
        "\n",
        "- df_nfe:\n",
        "  - CreateDate\n",
        "  - EmissionDate\n",
        "  - NFeNumber\n",
        "  - Discount\n",
        "  - EmissionDate\n",
        "\n",
        "O join entre as tabelas pode ser feito através das colunas CreateDate e NFeNumber, optei por manter duas colunas para o join pois em um ambiente de muitas requisições podemos ter duas requisições no mesmo timestamp e neste cenário fazer o join apenas pela coluna de CreateDate geraria um problema"
      ],
      "metadata": {
        "id": "ZlbElN6M3GrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data)\n",
        "df_itens = df.select(df.CreateDate, df.NFeNumber, F.explode(df.ItemList))\n",
        "\n",
        "df_itens = df_itens.withColumn(\"Value\", df_itens.col.getItem(\"Value\")) \\\n",
        "        .withColumn(\"Quantity\", df_itens.col.getItem(\"Quantity\")) \\\n",
        "        .withColumn(\"ProductName\", df_itens.col.getItem(\"ProductName\")) \\\n",
        "        .drop(\"col\")\n",
        "\n",
        "df_itens.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC3jmbLk3IXF",
        "outputId": "6e1550f7-2113-44ba-ec90-159f8a745688"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+---------+-----+--------+------------+\n",
            "|CreateDate            |NFeNumber|Value|Quantity|ProductName |\n",
            "+----------------------+---------+-----+--------+------------+\n",
            "|2021-05-24T20:21:34.79|501      |35.55|2       |Rice        |\n",
            "|2021-05-24T20:21:34.79|501      |11.55|5       |Flour       |\n",
            "|2021-05-24T20:21:34.79|501      |27.15|7       |Bean        |\n",
            "|2021-05-24T20:21:34.79|502      |12.25|10      |Tomate      |\n",
            "|2021-05-24T20:21:34.79|502      |7.55 |5       |Pasta       |\n",
            "|2021-05-24T20:21:34.79|503      |9.0  |6       |Beer        |\n",
            "|2021-05-24T20:21:34.79|503      |10.99|2       |French fries|\n",
            "|2021-05-24T20:21:34.79|503      |27.15|1       |Ice cream   |\n",
            "+----------------------+---------+-----+--------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_nf = df.select(df.CreateDate, df.NFeNumber, df.NFeID, df.Discount, df.EmissionDate)\n",
        "df_nf.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qktC_xlbdAhy",
        "outputId": "68683ada-5167-4052-d5e5-4b7f23b1d6c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------+-----+--------+-------------------+\n",
            "|          CreateDate|NFeNumber|NFeID|Discount|       EmissionDate|\n",
            "+--------------------+---------+-----+--------+-------------------+\n",
            "|2021-05-24T20:21:...|      501|    1|     0.0|2021-05-24T00:00:00|\n",
            "|2021-05-24T20:21:...|      502|    2|     0.0|2021-05-24T00:00:00|\n",
            "|2021-05-24T20:21:...|      503|    3|     0.0|2021-05-24T00:00:00|\n",
            "+--------------------+---------+-----+--------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entregavel 4"
      ],
      "metadata": {
        "id": "-38HoNfK5ut9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W3lkpAh85j7T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}